\chapter{Background}
\label{sec::ball}
Hierarchies or in other words trees can be expressed by ball embeddings. The largest N dimensional sphere corresponds to the root node of the tree. The direct children of the root node are represented by smaller spheres that are contained within the sphere of the root node. Hierarchies are an important part of natural language and signify a type of relationship with another word. For example sparrow, duck or Eagle are all of the type bird. In linguistics this is called a hypernym and hyponym relationship where the type of, in our example the bird, would be the hypernym and the concrete realizations of that type, in our example sparrow or a duck, would be some of the hyponyms of bird. It is evident that having this kind of hierarchical information can be useful when creating a rule-based natural language processing tool. Unlike stochastic or learning based methods hierarchical structures do not have error term to be minimized. Hierarchical structures are deterministic meaning we can always determine if a note is a child (hyponym) of a word that we are interested in or not. For example if we were to write a tool that needs to identifies cities within a certain region then the results certainly have to be direct or indirect hypnonyms of the word city.

\par
While rule-based natural language processing methods have been studied and applied for many decades recent machine learning algorithms, specifically deep neural networks, have produced stunning results that seemed almost impossible with the traditional rule-based approaches. However to actually use words or sentences in a neural network we first have to convert the input into to a vector that we can train our model on. This was first popularized in the word2vec paper \cite{mikolov2013efficient}. The goal is to create a dense vector representation for each word. The authors start off by creating a sparse representation also known as one hot encoding. For this they determine all words used in their text corpus and create a vocabulary from that. Then every word can be represented by a vector of the length of the vocabulary that only contains a single entry of 1 and all of the other entries are 0. This allows us, though in a very inefficient way, to feed any word or even a sentence, a concatenation of these vectors, into a neural network. Further they assume that similar words have a similar context. For example in "Can you *blank* me the train station." we can fill this gap with \textit{lead, guide,show} and all of them express a similar meaning. We can then train a neural network to represent every word with a limited dimension of our choosing. Common vector dimensions are 50, 100, 300, 500. This works remarkably well and allows us to explore relationships beyond simple hierarchies. Though it should be noted that sub word discretization methods exist \cite{DBLP:journals/corr/abs-1801-06146}. In  \cite{mikolov2013efficient} they could use the word vectors for simple arithmetics, one example of this would be Paris-France+ Italy=Rome. This approach has been extended multiple times, one such instance are the GloVe word embeddings \cite{pennington-etal-2014-glove}. As with all learning based methods there is no guarantee that certain relationships that we observe in our language are actually projected into the word vector space. 



N-Ball embeddings combine both approaches and fuse the reliability of a hierarchical based system with the power and flexibility that  word vectors provide. This is achieved by extending the word vectors and conceptually adding a sphere to each of them. In the next step we need to ensure that the word embeddings actually adhered to the hierarchical structure that we want to embed into them while preserving the vector that has been assigned by the word embedding of our choice. This is done by adding more dimensions to these word vectors and then only performing homothetic transformations on them.